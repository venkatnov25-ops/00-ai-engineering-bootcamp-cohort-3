{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d764c2c2",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4c26db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Prefetch, Filter, FieldCondition, MatchText, FusionQuery, Document\n",
    "\n",
    "\n",
    "from langsmith import traceable, get_current_run_tree\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.types import Send, Command\n",
    "\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "from jinja2 import Template\n",
    "from typing import Literal, Dict, Any, Annotated, List, Optional, Sequence\n",
    "from IPython.display import Image, display\n",
    "from operator import add\n",
    "from openai import OpenAI\n",
    "\n",
    "import openai\n",
    "\n",
    "import random\n",
    "import ast\n",
    "import inspect\n",
    "import instructor\n",
    "import json\n",
    "\n",
    "from utils.utils import get_tool_descriptions, format_ai_message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d162a",
   "metadata": {},
   "source": [
    "### This is about conditionally routing [based on router node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da39d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(BaseModel):\n",
    "    expanded_query: List[str] = []\n",
    "    retrieved_context: Annotated[List[str], add] = []\n",
    "    ## This is the addition for router test.\n",
    "    initial_query: str = \"\"\n",
    "    answer: str = \"\"\n",
    "    query: str = \"\"\n",
    "    k: int = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2714b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryExpandResponse(BaseModel):\n",
    "   expanded_query: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f41e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(\n",
    "    name=\"query_expand_node\",\n",
    "    run_type=\"llm\",\n",
    "    metadata={\"ls_provider\": \"openai\", \"ls_model_name\": \"gpt-4.1-mini\"}\n",
    ")\n",
    "def query_expand_node(state: State) -> dict:\n",
    "\n",
    "   prompt_template =  \"\"\"You are part of a shopping assistant that can answer questions about products in stock.\n",
    "\n",
    "Instructions:\n",
    "- You will be given a question and you need to expand it into a list of statements that can be used in contextual search to retrieve relevant products.\n",
    "- The statements should not overlap in context.\n",
    "- The answer to the question should contain detailed information about the product and returned with detailed specification in bullet points.\n",
    "\n",
    "<Question>\n",
    "{{ query }}\n",
    "</Question>\n",
    "\"\"\"\n",
    "\n",
    "   template = Template(prompt_template)\n",
    "   \n",
    "   prompt = template.render(\n",
    "      query=state.initial_query\n",
    "   )\n",
    "\n",
    "   client = instructor.from_openai(OpenAI())\n",
    "\n",
    "   response, raw_response = client.chat.completions.create_with_completion(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        response_model=QueryExpandResponse,\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=0.5,\n",
    "   )\n",
    "\n",
    "   return {\n",
    "      \"expanded_query\": response.expanded_query\n",
    "   }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76045f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_expand_conditional_edges(state: State):\n",
    "\n",
    "    send_messages = []\n",
    "\n",
    "    for query in state.expanded_query:\n",
    "        send_messages.append(\n",
    "            Send(\n",
    "                \"retrieve_node\",\n",
    "                {\n",
    "                    \"query\": query,\n",
    "                    \"k\": 10\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return send_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144fe332",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(\n",
    "    name=\"embed_query\",\n",
    "    run_type=\"embedding\",\n",
    "    metadata={\"ls_provider\": \"openai\", \"ls_model_name\": \"text-embedding-3-small\"}\n",
    ")\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    response = openai.embeddings.create(\n",
    "        input=[text],\n",
    "        model=model,\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "@traceable(\n",
    "    name=\"retrieve_top_n\",\n",
    "    run_type=\"retriever\"\n",
    ")\n",
    "def retrieve_node(state: State) -> dict:\n",
    "\n",
    "    qdrant_client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "    query_embedding = get_embedding(state[\"query\"])\n",
    "\n",
    "    results = qdrant_client.query_points(\n",
    "        collection_name=\"Amazon-items-collection-01-hybrid-search\",\n",
    "        prefetch=[\n",
    "            Prefetch(\n",
    "                query=query_embedding,\n",
    "                using=\"text-embedding-3-small\",\n",
    "                limit=20\n",
    "            ),\n",
    "            Prefetch(\n",
    "                query=Document(\n",
    "                    text=state[\"query\"],\n",
    "                    model=\"qdrant/bm25\"\n",
    "                ),\n",
    "                using=\"bm25\",\n",
    "                limit=20\n",
    "            )\n",
    "        ],\n",
    "        query=FusionQuery(fusion=\"rrf\"),\n",
    "        limit=state[\"k\"],\n",
    "    )\n",
    "\n",
    "    retrieved_context_ids = []\n",
    "    retrieved_context = []\n",
    "    similarity_scores = []\n",
    "    retrieved_context_ratings = []\n",
    "\n",
    "    for result in results.points:\n",
    "        retrieved_context_ids.append(result.payload[\"parent_asin\"])\n",
    "        retrieved_context.append(result.payload[\"description\"])\n",
    "        retrieved_context_ratings.append(result.payload[\"average_rating\"])\n",
    "        similarity_scores.append(result.score)\n",
    "\n",
    "    formatted_context = \"\"\n",
    "\n",
    "    for id, chunk, rating in zip(retrieved_context_ids, retrieved_context, retrieved_context_ratings):\n",
    "        formatted_context += f\"- ID: {id}, rating: {rating}, description: {chunk}\\n\"\n",
    "\n",
    "    return {\n",
    "        \"retrieved_context\": [formatted_context]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f59ca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregatorResponse(BaseModel):\n",
    "    answer: str = Field(description=\"Answer to the question.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c57f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(\n",
    "    name=\"aggregator_node\",\n",
    "    run_type=\"llm\",\n",
    "    metadata={\"ls_provider\": \"openai\", \"ls_model_name\": \"gpt-4.1-mini\"}\n",
    ")\n",
    "def aggregator_node(state: State) -> dict:\n",
    "\n",
    "   preprocessed_context = \"\\n\".join(state.retrieved_context)\n",
    "\n",
    "   prompt_template =  \"\"\"You are a shopping assistant that can answer questions about the products in stock.\n",
    "\n",
    "You will be given a question and a list of context.\n",
    "\n",
    "Instructions:\n",
    "- You need to answer the question based on the provided context only.\n",
    "- Never use word context and refer to it as the available products.\n",
    "- The answer to the question should contain detailed information about the product and returned with detailed specification in bullet points.\n",
    "\n",
    "Context:\n",
    "{{ preprocessed_context }}\n",
    "\n",
    "Question:\n",
    "{{ question }}\n",
    "\"\"\"\n",
    "\n",
    "   template = Template(prompt_template)\n",
    "   \n",
    "   prompt = template.render(\n",
    "      preprocessed_context=preprocessed_context,\n",
    "      question=state.initial_query\n",
    "   )\n",
    "\n",
    "   client = instructor.from_openai(OpenAI())\n",
    "\n",
    "   response, raw_response = client.chat.completions.create_with_completion(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        response_model=AggregatorResponse,\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=0.5,\n",
    "   )\n",
    "\n",
    "   return {\n",
    "      \"answer\": response.answer\n",
    "   }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb236dfd",
   "metadata": {},
   "source": [
    "## User Intent Query Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faccd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentRouterResponse(BaseModel):\n",
    "    question_relevant: bool\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b461c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(\n",
    "    name=\"agent_node\",\n",
    "    run_type=\"llm\",\n",
    "    metadata={\"ls_provider\": \"openai\", \"ls_model_name\": \"gpt-4.1-mini\"}\n",
    ")\n",
    "def intent_router_node(state: State):\n",
    "\n",
    "   prompt_template =  \"\"\"You are part of a shopping assistant that can answer questions about products in stock.\n",
    "\n",
    "Instructions:\n",
    "- You will be given a question and you need to clasify it into relevant or not relevant.\n",
    "- If the question is not relevant, return False in field \"question_relevant\" and set \"answer\" to explanation why it is not relevant.\n",
    "- If the question is relevant, return True in field \"question_relevant\" and set \"answer\" to \"\".\n",
    "- You should only answer questions about the products in stock. If the question is not about the products in stock, you should ask for clarification.\n",
    "\n",
    "<Question>\n",
    "{{ query }}\n",
    "</Question>\n",
    "\"\"\"\n",
    "\n",
    "   template = Template(prompt_template)\n",
    "   \n",
    "   prompt = template.render(\n",
    "      query=state.initial_query\n",
    "   )\n",
    "\n",
    "   client = instructor.from_openai(OpenAI())\n",
    "\n",
    "   response, raw_response = client.chat.completions.create_with_completion(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        response_model=IntentRouterResponse,\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=0.5,\n",
    "   )\n",
    "\n",
    "   return {\n",
    "      \"question_relevant\": response.question_relevant,\n",
    "      \"answer\": response.answer\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb6a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add conditional edge for router\n",
    "def intent_router_conditional_edges(state: State):\n",
    "\n",
    "    if state.question_relevant:\n",
    "        return \"query_expand_node\"\n",
    "    else:\n",
    "        return \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8b654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"query_expand_node\", query_expand_node)\n",
    "workflow.add_node(\"retrieve_node\", retrieve_node)\n",
    "workflow.add_node(\"aggregator_node\", aggregator_node)\n",
    "## Add intent router node\n",
    "workflow.add_node(\"intent_router_node\", intent_router_node)\n",
    "\n",
    "workflow.add_edge(START, \"intent_router_node\")\n",
    "workflow.add_conditional_edges(\"query_expand_node\", query_expand_conditional_edges)\n",
    "## Connect intent router node to query expand node or END based on condition in intent router node\n",
    "workflow.add_conditional_edges(\n",
    "    \"intent_router_node\",\n",
    "    intent_router_conditional_edges,\n",
    "    {\n",
    "        \"query_expand_node\": \"query_expand_node\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"retrieve_node\", \"aggregator_node\")\n",
    "workflow.add_edge(\"aggregator_node\", END)\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813f2af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6483c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is about relevancy yes\n",
    "\n",
    "initial_state = {\n",
    "    \"initial_query\": \"Can I get a tablet for my kid, a watch for me a laptop for my wife and a waterproof speaker for our party next week?\"\n",
    "}\n",
    "result = graph.invoke(initial_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687d03f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7483557",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Even if one is not relevant, it says 'yes' as others are relevant.\n",
    "print(result[\"answer\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a3354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {\n",
    "    \"initial_query\": \"Can I get a tablet for my kid, a watch for me a laptop for my wife and dishwasher tablet for my dishwasher?\"\n",
    "}\n",
    "result = graph.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f4a832",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b5a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1884464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_state = {\n",
    "    \"initial_query\": \"Whats the weather today?\"\n",
    "}\n",
    "result = graph.invoke(initial_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73857db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
