{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d178160b",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd299d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Prefetch, Filter, FieldCondition, MatchText, FusionQuery, Document\n",
    "\n",
    "\n",
    "from langsmith import traceable, get_current_run_tree\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.types import Send, Command\n",
    "\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "from jinja2 import Template\n",
    "from typing import Literal, Dict, Any, Annotated, List, Optional, Sequence\n",
    "from IPython.display import Image, display\n",
    "from operator import add\n",
    "from openai import OpenAI\n",
    "\n",
    "import openai\n",
    "\n",
    "import random\n",
    "import ast\n",
    "import inspect\n",
    "import instructor\n",
    "import json\n",
    "\n",
    "from utils.utils import get_tool_descriptions, format_ai_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c5c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = QdrantClient(url=\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea9b04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    response = openai.embeddings.create(\n",
    "        input=[text],\n",
    "        model=model,\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e504945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_data(query, qdrant_client, k=5):\n",
    "\n",
    "    query_embedding = get_embedding(query)\n",
    "\n",
    "    results = qdrant_client.query_points(\n",
    "        collection_name=\"Amazon-items-collection-01-hybrid-search\",\n",
    "        prefetch=[\n",
    "            Prefetch(\n",
    "                query=query_embedding,\n",
    "                using=\"text-embedding-3-small\",\n",
    "                limit=20\n",
    "            ),\n",
    "            Prefetch(\n",
    "                query=Document(\n",
    "                    text=query,\n",
    "                    model=\"qdrant/bm25\"\n",
    "                ),\n",
    "                using=\"bm25\",\n",
    "                limit=20\n",
    "            )\n",
    "        ],\n",
    "        query=FusionQuery(fusion=\"rrf\"),\n",
    "        limit=k,\n",
    "    )\n",
    "\n",
    "    retrieved_context_ids = []\n",
    "    retrieved_context = []\n",
    "    similarity_scores = []\n",
    "    retrieved_context_ratings = []\n",
    "\n",
    "    for result in results.points:\n",
    "        retrieved_context_ids.append(result.payload[\"parent_asin\"])\n",
    "        retrieved_context.append(result.payload[\"description\"])\n",
    "        retrieved_context_ratings.append(result.payload[\"average_rating\"])\n",
    "        similarity_scores.append(result.score)\n",
    "\n",
    "    return {\n",
    "        \"retrieved_context_ids\": retrieved_context_ids,\n",
    "        \"retrieved_context\": retrieved_context,\n",
    "        \"retrieved_context_ratings\": retrieved_context_ratings,\n",
    "        \"similarity_scores\": similarity_scores,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d488c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can I get a tablet?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f57bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = retrieve_data(query, qdrant_client, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bb162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e4591a",
   "metadata": {},
   "source": [
    "### Multi-Intent Queries ==> Totally unrelated Query parts but embedded in one question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f69010",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can I get a tablet for my kid, a watch for me and a laptop for my wife?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d2640",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = retrieve_data(query, qdrant_client, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa7f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02528d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryExpandResponse(BaseModel):\n",
    "   statements: List[str]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2633c83c",
   "metadata": {},
   "source": [
    "### We need to decompose query and into statements. This is where prompt chaining comes into place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa0489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_expand_node(query) -> dict:\n",
    "\n",
    "\n",
    "### Pl note it. Query should be expanded into statements and they should not overlap in context.\n",
    "   prompt_template =  \"\"\"You are part of a shopping assistant that can answer questions about products in stock.\n",
    "\n",
    "Instructions:\n",
    "- You will be given a question and you need to expand it into a list of statements that can be used in contextual search to retrieve relevant products.\n",
    "- The statements should not overlap in context.\n",
    "\n",
    "<Question>\n",
    "{{ query }}\n",
    "</Question>\n",
    "\"\"\"\n",
    "\n",
    "   template = Template(prompt_template)\n",
    "   \n",
    "   prompt = template.render(\n",
    "      query=query\n",
    "   )\n",
    "\n",
    "   client = instructor.from_openai(OpenAI())\n",
    "\n",
    "   response, raw_response = client.chat.completions.create_with_completion(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        response_model=QueryExpandResponse,\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=0.5,\n",
    "   )\n",
    "\n",
    "   return {\n",
    "      \"queries\": response.statements\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c094b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = query_expand_node(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810f1829",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ba7358",
   "metadata": {},
   "source": [
    "### Integration with LangGraph for prompt chaining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84a85b6",
   "metadata": {},
   "source": [
    "### Query Expansion (Sequential Execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca6b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graph state that is mutable and appendable\n",
    "\n",
    "class State(BaseModel):\n",
    "    expanded_query: List[str] = []\n",
    "    retrieved_context: Annotated[List[str], add] = []\n",
    "    initial_query: str = \"\"\n",
    "    answer: str = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1121e2af",
   "metadata": {},
   "source": [
    "### Query Expansion / Rewriting Node (Agent node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c69858",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryExpandResponse(BaseModel):\n",
    "   expanded_query: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cf6786",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(\n",
    "    name=\"query_expand_node\",\n",
    "    run_type=\"llm\",\n",
    "    metadata={\"ls_provider\": \"openai\", \"ls_model_name\": \"gpt-4.1-mini\"}\n",
    ")\n",
    "### It takes in a state, not a query (Part of LangGraph graph)\n",
    "def query_expand_node(state: State) -> dict:\n",
    "\n",
    "   prompt_template =  \"\"\"You are part of a shopping assistant that can answer questions about products in stock.\n",
    "\n",
    "Instructions:\n",
    "- You will be given a question and you need to expand it into a list of statements that can be used in contextual search to retrieve relevant products.\n",
    "- The statements should not overlap in context.\n",
    "- Be as concise as possible, do not make up synonyms for statements, one statement per piece of context.\n",
    "\n",
    "<Question>\n",
    "{{ query }}\n",
    "</Question>\n",
    "\"\"\"\n",
    "\n",
    "   template = Template(prompt_template)\n",
    "   \n",
    "   prompt = template.render(\n",
    "      query=state.initial_query\n",
    "   )\n",
    "\n",
    "   client = instructor.from_openai(OpenAI())\n",
    "\n",
    "   response, raw_response = client.chat.completions.create_with_completion(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        response_model=QueryExpandResponse,\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=0.5,\n",
    "   )\n",
    "\n",
    "   return {\n",
    "      \"expanded_query\": response.expanded_query\n",
    "   }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40197319",
   "metadata": {},
   "source": [
    "#### Retriever Agent Node (Retrieve information with the decomposed query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3ded32",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(\n",
    "    name=\"embed_query\",\n",
    "    run_type=\"embedding\",\n",
    "    metadata={\"ls_provider\": \"openai\", \"ls_model_name\": \"text-embedding-3-small\"}\n",
    ")\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    response = openai.embeddings.create(\n",
    "        input=[text],\n",
    "        model=model,\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "@traceable(\n",
    "    name=\"retrieve_top_n\",\n",
    "    run_type=\"retriever\"\n",
    ")\n",
    "def retrieve_data(query, k=5):\n",
    "\n",
    "    qdrant_client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "    query_embedding = get_embedding(query)\n",
    "\n",
    "    results = qdrant_client.query_points(\n",
    "        collection_name=\"Amazon-items-collection-01-hybrid-search\",\n",
    "        prefetch=[\n",
    "            Prefetch(\n",
    "                query=query_embedding,\n",
    "                using=\"text-embedding-3-small\",\n",
    "                limit=20\n",
    "            ),\n",
    "            Prefetch(\n",
    "                query=Document(\n",
    "                    text=query,\n",
    "                    model=\"qdrant/bm25\"\n",
    "                ),\n",
    "                using=\"bm25\",\n",
    "                limit=20\n",
    "            )\n",
    "        ],\n",
    "        query=FusionQuery(fusion=\"rrf\"),\n",
    "        limit=k,\n",
    "    )\n",
    "\n",
    "    retrieved_context_ids = []\n",
    "    retrieved_context = []\n",
    "    similarity_scores = []\n",
    "    retrieved_context_ratings = []\n",
    "\n",
    "    for result in results.points:\n",
    "        retrieved_context_ids.append(result.payload[\"parent_asin\"])\n",
    "        retrieved_context.append(result.payload[\"description\"])\n",
    "        retrieved_context_ratings.append(result.payload[\"average_rating\"])\n",
    "        similarity_scores.append(result.score)\n",
    "\n",
    "    formatted_context = \"\"\n",
    "\n",
    "    for id, chunk, rating in zip(retrieved_context_ids, retrieved_context, retrieved_context_ratings):\n",
    "        formatted_context += f\"- ID: {id}, rating: {rating}, description: {chunk}\\n\"\n",
    "\n",
    "    return formatted_context\n",
    "\n",
    "\n",
    "## This is the agent node that calls each decomposed query and returns the context\n",
    "@traceable(\n",
    "    name=\"retriever_node\",\n",
    "    run_type=\"retriever\"\n",
    ")\n",
    "def retriever_node(state: State) -> dict:\n",
    "\n",
    "    retrieved_context = []\n",
    "\n",
    "    for query in state.expanded_query:\n",
    "        retrieved_context.append(retrieve_data(query, k=5))\n",
    "\n",
    "    return {\n",
    "        \"retrieved_context\": retrieved_context\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ebf1e2",
   "metadata": {},
   "source": [
    "### Aggregator Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f119485",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregatorResponse(BaseModel):\n",
    "    answer: str = Field(description=\"Answer to the question.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9211f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(\n",
    "    name=\"aggregator_node\",\n",
    "    run_type=\"llm\",\n",
    "    metadata={\"ls_provider\": \"openai\", \"ls_model_name\": \"gpt-4.1-mini\"}\n",
    ")\n",
    "def aggregator_node(state: State) -> dict:\n",
    "\n",
    "   preprocessed_context = \"\\n\".join(state.retrieved_context)\n",
    "\n",
    "   prompt_template =  \"\"\"You are a shopping assistant that can answer questions about the products in stock.\n",
    "\n",
    "You will be given a question and a list of context.\n",
    "\n",
    "Instructions:\n",
    "- You need to answer the question based on the provided context only.\n",
    "- Never use word context and refer to it as the available products.\n",
    "- The answer to the question should contain detailed information about the product and returned with detailed specification in bullet points.\n",
    "\n",
    "Context:\n",
    "{{ preprocessed_context }}\n",
    "\n",
    "Question:\n",
    "{{ question }}\n",
    "\"\"\"\n",
    "\n",
    "   template = Template(prompt_template)\n",
    "   \n",
    "   prompt = template.render(\n",
    "      preprocessed_context=preprocessed_context,\n",
    "      question=state.initial_query\n",
    "   )\n",
    "\n",
    "   client = instructor.from_openai(OpenAI())\n",
    "\n",
    "   response, raw_response = client.chat.completions.create_with_completion(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        ### Response streamed into a Pydantic object\n",
    "        response_model=AggregatorResponse,\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=0.5,\n",
    "   )\n",
    "\n",
    "   return {\n",
    "      \"answer\": response.answer\n",
    "   }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dabb038",
   "metadata": {},
   "source": [
    "### Graph construction (Sequential with retriever agent node doing in loop for each of the query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0022e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prompt chaining - sequential graph\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"query_expand_node\", query_expand_node)\n",
    "workflow.add_node(\"retriever_node\", retriever_node)\n",
    "workflow.add_node(\"aggregator_node\", aggregator_node)\n",
    "\n",
    "workflow.add_edge(START, \"query_expand_node\")\n",
    "workflow.add_edge(\"query_expand_node\", \"retriever_node\")\n",
    "\n",
    "workflow.add_edge(\"retriever_node\", \"aggregator_node\")\n",
    "workflow.add_edge(\"aggregator_node\", END)\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b81f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43039f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can I get a tablet for my kid, a watch for me and a laptop for my wife?\"\n",
    "initial_state = {\n",
    "    \"initial_query\": query,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83411cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = graph.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2769f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c7586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e79e12",
   "metadata": {},
   "source": [
    "## Now let's run the retriever node in parallel per individual query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17def8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(BaseModel):\n",
    "    expanded_query: List[str] = []\n",
    "    ## This is an append because of \"add\" and hence retrieved context returned per query will be appended here.\n",
    "    retrieved_context: Annotated[List[str], add] = []\n",
    "    initial_query: str = \"\"\n",
    "    answer: str = \"\"\n",
    "    query: str = \"\"\n",
    "    k: int = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5a9389",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QueryExpandResponse(BaseModel):\n",
    "   expanded_query: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d1f4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(\n",
    "    name=\"query_expand_node\",\n",
    "    run_type=\"llm\",\n",
    "    metadata={\"ls_provider\": \"openai\", \"ls_model_name\": \"gpt-4.1-mini\"}\n",
    ")\n",
    "def query_expand_node(state: State) -> dict:\n",
    "\n",
    "   prompt_template =  \"\"\"You are part of a shopping assistant that can answer questions about products in stock.\n",
    "\n",
    "Instructions:\n",
    "- You will be given a question and you need to expand it into a list of statements that can be used in contextual search to retrieve relevant products.\n",
    "- The statements should not overlap in context.\n",
    "- The answer to the question should contain detailed information about the product and returned with detailed specification in bullet points.\n",
    "\n",
    "<Question>\n",
    "{{ query }}\n",
    "</Question>\n",
    "\"\"\"\n",
    "\n",
    "   template = Template(prompt_template)\n",
    "   \n",
    "   prompt = template.render(\n",
    "      ## Take it from state as it is a node\n",
    "      query=state.initial_query\n",
    "   )\n",
    "\n",
    "   client = instructor.from_openai(OpenAI())\n",
    "\n",
    "   response, raw_response = client.chat.completions.create_with_completion(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        response_model=QueryExpandResponse,\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=0.5,\n",
    "   )\n",
    "\n",
    "   return {\n",
    "      \"expanded_query\": response.expanded_query\n",
    "   }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1869590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_expand_conditional_edges(state: State):\n",
    "\n",
    "    send_messages = []\n",
    "\n",
    "    for query in state.expanded_query:\n",
    "        send_messages.append(\n",
    "            ## Broadcast work to specific nodes (This is LangGraph construct)\n",
    "            Send(\n",
    "                ## Node\n",
    "                \"retriever_node\",\n",
    "                ## Local state - Query\n",
    "                {\n",
    "                    \"query\": query,\n",
    "                    \"k\": 10\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return send_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31220ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(\n",
    "    name=\"embed_query\",\n",
    "    run_type=\"embedding\",\n",
    "    metadata={\"ls_provider\": \"openai\", \"ls_model_name\": \"text-embedding-3-small\"}\n",
    ")\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    response = openai.embeddings.create(\n",
    "        input=[text],\n",
    "        model=model,\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "@traceable(\n",
    "    name=\"retrieve_top_n\",\n",
    "    run_type=\"retriever\"\n",
    ")\n",
    "def retriever_node(state: State) -> dict:\n",
    "\n",
    "    qdrant_client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "    ## This query is now from the state where individual query is broadcast\n",
    "    query_embedding = get_embedding(state[\"query\"])\n",
    "\n",
    "    results = qdrant_client.query_points(\n",
    "        collection_name=\"Amazon-items-collection-01-hybrid-search\",\n",
    "        prefetch=[\n",
    "            Prefetch(\n",
    "                query=query_embedding,\n",
    "                using=\"text-embedding-3-small\",\n",
    "                limit=20\n",
    "            ),\n",
    "            Prefetch(\n",
    "                query=Document(\n",
    "                    text=query,\n",
    "                    model=\"qdrant/bm25\"\n",
    "                ),\n",
    "                using=\"bm25\",\n",
    "                limit=20\n",
    "            )\n",
    "        ],\n",
    "        query=FusionQuery(fusion=\"rrf\"),\n",
    "        limit=state[\"k\"],\n",
    "    )\n",
    "\n",
    "    retrieved_context_ids = []\n",
    "    retrieved_context = []\n",
    "    similarity_scores = []\n",
    "    retrieved_context_ratings = []\n",
    "\n",
    "    for result in results.points:\n",
    "        retrieved_context_ids.append(result.payload[\"parent_asin\"])\n",
    "        retrieved_context.append(result.payload[\"description\"])\n",
    "        retrieved_context_ratings.append(result.payload[\"average_rating\"])\n",
    "        similarity_scores.append(result.score)\n",
    "\n",
    "    formatted_context = \"\"\n",
    "\n",
    "    for id, chunk, rating in zip(retrieved_context_ids, retrieved_context, retrieved_context_ratings):\n",
    "        formatted_context += f\"- ID: {id}, rating: {rating}, description: {chunk}\\n\"\n",
    "\n",
    "    return {\n",
    "        \"retrieved_context\": [formatted_context]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d16315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregatorResponse(BaseModel):\n",
    "    answer: str = Field(description=\"Answer to the question.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7a6319",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(\n",
    "    name=\"aggregator_node\",\n",
    "    run_type=\"llm\",\n",
    "    metadata={\"ls_provider\": \"openai\", \"ls_model_name\": \"gpt-4.1-mini\"}\n",
    ")\n",
    "def aggregator_node(state: State) -> dict:\n",
    "\n",
    "   preprocessed_context = \"\\n\".join(state.retrieved_context)\n",
    "\n",
    "   prompt_template =  \"\"\"You are a shopping assistant that can answer questions about the products in stock.\n",
    "\n",
    "You will be given a question and a list of context.\n",
    "\n",
    "Instructions:\n",
    "- You need to answer the question based on the provided context only.\n",
    "- Never use word context and refer to it as the available products.\n",
    "- The answer to the question should contain detailed information about the product and returned with detailed specification in bullet points.\n",
    "\n",
    "Context:\n",
    "{{ preprocessed_context }}\n",
    "\n",
    "Question:\n",
    "{{ question }}\n",
    "\"\"\"\n",
    "\n",
    "   template = Template(prompt_template)\n",
    "   \n",
    "   prompt = template.render(\n",
    "      preprocessed_context=preprocessed_context,\n",
    "      question=state.initial_query\n",
    "   )\n",
    "\n",
    "   client = instructor.from_openai(OpenAI())\n",
    "\n",
    "   response, raw_response = client.chat.completions.create_with_completion(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        response_model=AggregatorResponse,\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=0.5,\n",
    "   )\n",
    "\n",
    "   return {\n",
    "      \"answer\": response.answer\n",
    "   }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c7a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"query_expand_node\", query_expand_node)\n",
    "workflow.add_node(\"retriever_node\", retriever_node)\n",
    "workflow.add_node(\"aggregator_node\", aggregator_node)\n",
    "\n",
    "workflow.add_edge(START, \"query_expand_node\")\n",
    "\n",
    "## We add conditional edges here (This broadcasts work for every query to retrieve node as defined in conditional edge)\n",
    "workflow.add_conditional_edges(\"query_expand_node\", query_expand_conditional_edges)\n",
    "\n",
    "workflow.add_edge(\"retriever_node\", \"aggregator_node\")\n",
    "workflow.add_edge(\"aggregator_node\", END)\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17fe2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e18651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can I get a tablet for my kid, a watch for me and a laptop for my wife?\"\n",
    "initial_state = {\n",
    "    \"initial_query\": query,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b73458",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3df116",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35009d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
